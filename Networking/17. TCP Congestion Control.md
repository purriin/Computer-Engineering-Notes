## 17.1 AIMD
- Approach: Senders can increase rate until packet loss (congestion) occurs, then decrease sending rate on loss event

**Additive Increase**: Increase sending rate by 1 maximum segment size every RTT until loss detected

**Multiplicative Decrease**: Cut sending rate in half at each loss event

- AIMD sawtooth behaviour: probing for bandwidth

**Multiplicative Decrease**: Sending rate is
- Cut in half on loss detected by triple duplicate ACK
- Cut to 1 MSS (maximum segment size) when loss detected by timeout

- Why AIMD?
	- AIMD: A distributed, asynchronous algorithm - has been shown to:
		- Optimize congested flow rates network wide
		- Have desirable stability properties

## 17.2 TCP Congestion Control Details
**TCP Sending Behaviour**:
- Roughly: send `cwnd` bytes, wait RTT for ACKS then send more bytes
$$
\text{TCP Rate } \approx \frac{cwnd}{RTT} \text{ bytes/sec}
$$
- TCP sender limits transmission: `LastByteSent-LastByteAcked <= cwnd`
- `cwd` is dynamically adjusted in response to observed network congestion (implementing TCP congestion control)

## 17.3 TCP Slow Start
- When connection begins, increase rate exponentially until first loss event
	- Initially `cwnd` = 1 MSS
	- Double `cwnd` every RTT
	- Done by incrementing `cwnd` for every ACK received
- Summary: Initial rate is slow but ramps up exponentially fast

Q: When should the exponential increase switch to linear?
A: When `cwnd` gets to 1/2 of its value before timeout

**Implementation**:
- Variable `ssthresh`
- On loss event `ssthresh` is set to 1/2 of `cwd` just before loss event

## 17.4 TCP CUBIC
- Is there a better way than AIMD to "probe" for usable bandwidth?
- Insight/Intuition:
	- $W_{max}$: sending rate at which congestion loss was detected
	- Congestion state of bottleneck link probably hasn't changed much
	- After cutting rate/window in half on loss initially ramp $W_{max}$ faster but then approach $W_{max}$ more slowly
- $K$: point in time when TCP window size will reach $W_{max}$
	- $K$ itself is tuneable
- Increase W as a function of the cube of the distance between current time and K
	- Larger increases when further away from K
- TCP CUBIC default in Linux most popular TCP for popular Web servers

## 17.5 TCP and the Congested "Bottleneck Link"
- TCP (Classic, CUBE) increase TCP's sending rate until packet loss occurs at some router's output: the bottleneck link

**TCP Throughput**
- Avg. TCP throughput as a function of window size and RTT?
	- Ignore slow start assume there is always data to send
- W: Window size (measured in bytes) where loss occurs
	- Avg window size (\# in flight bytes) is 3/4 W
	- Avg Throughput is 3/4 W per RTT

## 17.6 TCP Over "Long, Fat Pipes"

## 17.7 TCP Fairness
**Fairness Goal**: If K TCP sessions share same bottleneck link of bandwidth R, each should have an average rate of R/K

**Is TCP Fair?**
- Yes under idealized assumptions:
	- Same RTT
	- Fixed number of sessions only in congestion avoidance

**Must all Network Apps be Fair**?
- Fairness and UDP
	- Multimedia apps often do not use TCP
		- Do not want rate throttled by congestion control
	- Instead use UDP:
		- Send audio/video at constant rate, tolerate packet loss
	- There is no "Internet police" policing use of congestion control
- Fairness, parallel TCP connections
	- Application can open multiple parallel connection between 2 hosts
	- Web browsers do this, e.g., link of rate R with 9 existing connections:
		- New app asks 1 TCP gets rate R/10
		- New app asks for 11 TCPs gets R/2

## 17.8 QUIC: Quick UDP Internet Connections
- Application layer protocol, on top of UDP
	- Increase performance of HTTP
	- Deployed on many Google servers, apps (Chrome, mobile Youtube apps)
- Adopts approaches we've studied n this chapter for connection establishment, error control, congestion control

**Error and Congestion Control**: "Readers familiar with TCP's loss detection and congestion control will find algorithms here that parallel well-known TCP ones"

**Connection Establishment**: Reliability, congestion control, authentication, encryption, state established in one RTT

- Multiple application-level "streams" multiplexed over single QUIC connection
	- Separate reliable data transfer, security
	- Common congestion control
