## 10.1 Peer-To-Peer (P2P) Architecture
- *No* always-on server
- Arbitrary end systems directly communicate
- Peers request service from other peers, provide service in return to other peers
	- Self Scalability: new peers bring new service capacity and new service demands
- Peers are intermittently connected and change IP addresses
	- Complex management
- Examples: P2P file sharing (BitTorrent), streaming (KanKan), VoIP (Skype)

## 10.2 File Distribution: Client Server vs. P2P
Q: How much time to distribute file (size F) from 1 server to N peers
- Peer upload / Download capacity is limited resource
### 10.2.1 File Distribution Time: Client-Server
**Server Transmission**: Must sequentially send (upload) $N$ file copies:
- Time to send 1 copy: $F/u_s$
- Time to send $N$ copies: $NF/u_s$

**Client**: Each client must download file copy
- $d_{min} =$ min client download rate
- Min client download time: $F/d_{min}$
$$
\begin{gathered}
	\text{Time to distribute F to N clients using Client-Server Approach} \\
	= \\
	D_{cs} \ge max\{\frac{NF}{u_s}, \frac{F}{d_{min}}\}
\end{gathered}
$$
- Increases linearly with $N$

### 10.2.2 File Distribution Time: P2P

![[Pasted image 20240131165142.png]]

**Server Transmission**: Must upload at least 1 copy
- Time to send 1 copy: $F/u_s$

**Client**: Each client must download file copy
- Min client download time: $F/d_{min}$

**Clients**: As aggregate must download $NF$ bits
- Max upload rate (limiting max download rate) is $u_s + \Sigma u_i$ 

$$
\begin{gathered}
	\text{Time to distribute F to N clients using P2P Approach} \\
	= \\
	D_{P2P} \ge max\{\frac{F}{u_s}, \frac{F}{d_{min}}, \frac{NF}{u_s + \Sigma u_i}\}
\end{gathered}
$$
- Increases linearly with $N$ but also does the denominator as each peer brings service capacity

![[Pasted image 20240131165459.png]]

## 10.3 P2P File Distribution: BitTorrent
- File divided into 256Kb chunks
- Peers in torrent send/receive file chunks

![[Pasted image 20240131165548.png]]

- Peer joining torrent:
	- Has no chunks, but will accumulate them over time from other peers
	- Registers with tracker to get list of peers, connects to subset of peers (“neighbors”)
- While downloading, peer uploads chunks to other peers
- Peer may change peers with whom it exchanges chunks 
- Churn: peers may come and go
- Once peer has entire file, it may (selfishly) leave or (altruistically) remain in torrent

**Requesting chunks**: 
- At any given time, different peers have different subsets of file chunks
- Periodically, Alice asks each peer for list of chunks that they have
- Alice requests missing chunks from peers, rarest first

**Sending chunks**: tit-for-tat
- Alice sends chunks to those four peers currently sending her chunks at highest rate
	- Other peers are choked by Alice (do not receive chunks from her)
	- Re-evaluate top 4 every10 secs
- Every 30 secs: randomly select another peer, starts sending chunks
	- “Optimistically unchoke” this peer
	- Newly chosen peer may join top 4

**BitTorrent: tit-for-tat** 
1. Alice “optimistically unchokes” Bob 
2. Alice becomes one of Bob’s top-four providers; Bob reciprocates
3. Bob becomes one of Alice’s top-four providers higher upload rate: find better trading partners, get file faster !

#### Centralized Approach:
- Bob registers his address and an ID for the file with the server
- Alice contacts the server to know who has the file
- Server sends Bob’s address to Alice
- Alice contacts Bob directly and gets the file.
- Used in Napster

**Pros**:
- Easy to find content 
- Quick to resolve the address
**Cons**:
- A central point of failure
- Performance bottleneck
- Legal issues

#### Fully Distributed (flooding): 
- Content can be found by flooding
- Not efficient in terms of bandwidth
- Can use limited flooding
	- May not find the content
- Gnutella used flooding
- Flooding is not scalable

## Distributed Hash Table (DHT) 
- We are looking for a distributed database and an efficient search algorithm.
- DHT = distributed P2P database
- Database has (key, value) pairs;
	- key: content type
	- value: IP address
- Peers query DB with key
- DB returns values that match the key
- Peers can also insert (key, value) peers

DHT Identifiers 
- Assign integer identifier to each peer in range \[0,2n-1].
	- Each identifier can be represented by n bits (usually 160 bits).
- Require each key to be an integer in same range.
- To get integer keys, hash original key.
	- eg, key = h(“Led Zeppelin IV”)
	- This is why they call it a distributed “hash” table

### How to assign keys to peers?
Central issue:
- Assigning (key, value) pairs to peers.
- Rule: assign key to the peer that has the closest ID.
- Convention in lecture: closest is the immediate successor of the key.
- Ex: n=4; peers: 1,3,4,5,8,10,12,14; 
	- key = 13, then successor peer = 14
	- key = 15, then successor peer = 1

### Circle DHT
- Each peer is only aware of immediate successor and predecessor.
- “Overlay network”
![[Pasted image 20240131170416.png]]
![[Pasted image 20240131170433.png]]

- Each peer keeps track of IP addresses of predecessor, successor, short cuts.
- Reduced from 6 to 2 messages.
- Possible to design shortcuts so O(log N) neighbors, O(log N) messages in query

### Peer Churn
- To handle peer churn, require each peer to know the IP address of its two successors.
- Each peer periodically pings its two successors to see if they are still alive.

- Peer 5 abruptly leaves
- Peer 4 detects; makes 8 its immediate successor; 
- asks 8 who its immediate successor is; 
- makes 8’s immediate successor its second successor.
![[Pasted image 20240131170504.png]]

## Peer Joining DHT
- Say peer 13 wants to join
- It sends a message to the ring (say peer 1) to find its successor and predecessor peers
- Message is propagated until it reaches peer 12
- 12 sends a message to 13 identifying itself and 15 as the predecessor and successor nodes

![[Pasted image 20240131170525.png]]

## Example
1. All keys are distributed uniformly at random in the key range. The query for any key is generated with the same probability. Each P2P link has a delay of 1 second. All 8 peers are responsible for the same number of queries on average. What is the average search time in the system?
![[Pasted image 20240131170852.png]]

$\text{Avg Delay} = \frac{1}{8}(0 + 2 + 3 + 4 + 5 + 6 + 7 + 8) = 35/8$
