## 7.1 Cookies - Maintaining User/Server State
- Recall: HTTP GET/response interaction is *stateless*
- No notion of multi-step exchanges of HTTP messages to complete a Web “transaction”
	- No need for client/server to track “state” of multi-step exchange
	- All HTTP requests are independent of each other
	- No need for client/server to “recover” from a partially-completed-but-never-completely-completed transaction

- Websites and client browsers use **cookies** to maintain some state between transactions
- 4 components:
	1. Cookie header line of HTTP response message 
	2. Cookie header line in next HTTP request message
	3. Cookie file kept on user’s host, managed by user’s browser
	4. Back-end database at Web site

- What cookies can be used for:
	- Authorization
	- Shopping carts
	- Recommendations
	- User session state (Web email)
- Aside: Cookies and privacy
	- Cookies permit sites to learn a lot about you on their site
	- Third party persistent cookies (tracking cookies) allow common identity (cookie value) to be tracked across multiple web sites
- Q: If the user is authenticated by using the username and password, do we still need to use cookies? Ans: It is useful to stay logged in

## 7.2 Web Caches (Proxy Servers)
**Goal**: Satisfy client request without involving origin server
- User configures browser to point to a Web cache
- Browser sends all HTTP requests to cache
	- If object in cache: cache returns object to client
	- Else cache requests object from origin server, caches received object, then returns object to client
- Web cache acts as both client and server
	- Server for original requesting client
	- Client to origin server
- Typically cache is installed by ISP (University, company, residential ISP)

**Why Web Caching?**
- Reduce response time for client request
	- Cache is closer to client
- Reduce traffic on an institution's access link
- Internet is dense with caches
	- Enables "poor" content providers to more effectively deliver content

### 7.2.1 Caching Examples
1. Scenario:
	- Access link rate: 1.54 Mbps
	- RTT from institutional router to server: 2 sec
	- Web object size: 100k bits
	- Average request rate from browsers to origin servers: 15/sec
		- Average data rate to browsers: 1.50 Mbps
	**Performance**:$$
		\text{LAN Utilization = } \frac{15 \times 100 \times 10^3}{10^9} = 0.15\%
		$$$$
		    \text{Access Link Utilization = } \frac{1.5 Mb/s}{1.54 Mb/s} = 0.97
		$$
	- 0.97 problem with large delays at high utilization$$
		\text{End-End Delay = Internet Delay + Access Link Delay + LAN Delay = 2 sec + minutes} + \mu \text{ secs} 
	$$
2. Calculating Access link utilization, end-end delay with cache:
	- Suppose cache hit rate is 0.4: 40% requests satisfied at cache, 60% requests satisfied at origin
	- Access link: 60% of requests use access link 
	- Data rate to browsers over access link = 0.6 * 1.50 Mbps = 0.9 Mbps ▪ utilization = 0.9/1.54 = 0.58
	- Average end-end delay = 0.6 * (delay from origin servers) + 0.4 * (delay when satisfied at cache) = 0.6 (2.01) + 0.4 (~msecs) = ~ 1.2 secs
	- Lower average end-end delay than with 154 Mbps link (and cheaper too!)

3. Assume instead of using a cache, we increase the rate of the outgoing link to 154 Mb/s. What would be the average total delay in fetching files?
$$
	LAN = \frac{15 \times 100k}{1G} = \frac{1.5M}{1G} = 0.15\%
	$$
$$
	\text{Access Link Utilization = } \frac{1.5M}{154M} = 0.97\%
	$$
- Delay is restricted to internet delay, i.e. 7 seconds
## 7.3 Conditional GET
**Goal**: Don't send object if cache has up-to-date cached version
- No object transmission delay
- Lower link utilization

- **Cache**: Specify date of cached copy in HTTP request 
	- `if-modified-since: <date>`
- **Server**: Response contains no object if cached copy is up-to-date:
	- `HTTP/1.0 304 Not Modified`
## 7.4 HTTP/2
**Key Goal**: Decreased delay in multi-object HTTP requests

**HTTP1.1**: Introduced multiple, pipelined GETs over single TCP connection
- Server responds *in-order* (FCFS: First-come-first-served scheduling) to GET requests
- With FCFS, small object may have to wait for transmission (head-of-line (HOL) blocking) behind large object(s)
- Loss recovery (retransmitting lost TCP segments) stalls object transmission
- Suffers from HOL blocking

**HTTP/2**: Increased flexibility at *server* in sending objects to client:
- Methods, status codes, most header files unchanged from HTTP1.1
- Transmission order of requested objects based on client-specific object priority (not necessarily FCFS)
- *Push* unrequested objects to client
- Divide objects into frames, schedule frames to mitigate HOL blocking

**HTTP/2 to HTTP/3**
- HTTP/2 over single TCP connection means:
	- Recovery from packet loss still stalls all object transmissions
		- As in HTTP1.1, browsers have incentive to open multiple parallel TCP connections to reduce stalling, increase overall throughput
	- No security over vanilla TCP connection
	- HTTP/3 adds security, per object error and congestion control (more pipelining) over UDP
		- More on HTTP/3 in transport layer